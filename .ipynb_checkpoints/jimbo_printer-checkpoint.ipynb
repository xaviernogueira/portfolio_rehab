{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f39fe4d",
   "metadata": {},
   "source": [
    "## **Portfolio Rehab:** An automated portfolio risk analyzer w/ stock pic recomendations\n",
    "\n",
    "Create a Jupyter notebook that outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7847ca4",
   "metadata": {},
   "source": [
    "## **Pitch:**\n",
    "- You are a long term invester who prefers to buy and hold positions \n",
    "- You like your positions, but are aware of the risks associated with the elevated prices in the current frothy market environment\n",
    "- You want insight into portfolio risks, and stategies to mitigate or hedge said risk to facilitate quality long term returns\n",
    "- You do not want to pay for expensive, monthly programs and would rather invest in a one time analysis report of your portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904b0bad",
   "metadata": {},
   "source": [
    "## **Inputs:**\n",
    "* Stock portfolio: tickers, holding volume, average price\n",
    "* Time horizon \n",
    "* Risk tolerance (Low / near retirement or sell point, medium / typical invester with stable income. high / higher income expected in the future)\n",
    "* Desired average annual return over the time horizon\n",
    "* % current margin use\n",
    "* Avoidance options: No fossil fuels, no China stocks, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc2382",
   "metadata": {},
   "source": [
    "## *Single stock analyses:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451293ce",
   "metadata": {},
   "source": [
    "### **Discounted Cash Flow (DCF) analysis**\n",
    "Here we apply Sven's discounted cash flow model using the\n",
    "\n",
    "#### Inputs\n",
    "- User: Desired annualized return / discount rate\n",
    "- User: Time horizon (T)\n",
    "- Historic terminal multiple series -- Used to calculate probability of different terminal multiple scenarios\n",
    "- Recent earnings (quaterly and annuals) \n",
    "\n",
    "Annual growth rates are shown to inform inital growth rate, which decreases by 50% every 5 years of T\n",
    "\n",
    "#### Outputs\n",
    "- Estimated intrinsic value for desired return after T in optimistic, middle, and pessimistic scenarios. Plotted vs predicted growth rates.\n",
    "- If estimated returns are lower that expected (i.e. value after DCF analysis is less tha\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c992395",
   "metadata": {},
   "source": [
    "### **Intrest rates risk analysis**\n",
    "#### Inputs\n",
    "- Calculate P/E, P/S, CAPE distribution in the portoflio\n",
    "- Calculate averages weighted by equity for the above values \n",
    "- Pull historic P/E, P/S, and CAPE where available and calculate correlation with intrest rates / liquidity \n",
    "\n",
    "#### Outputs\n",
    "- To what degree are position valuations negatively correlated with US treasury intrest rates\n",
    "    * How does correlation coefficient and R^2 compare to S&P broadly \n",
    "    * Describe comparison to market as well as correlation qualititively in a string output\n",
    "    * Make graphs showing P/E, P/S overlayed w/ intrest rates\n",
    "- To what degree are average valuations predicted to mean revert, compare to sector means as well as market means via standard deviations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a751bd-c11f-4563-87e5-a4cb722da44b",
   "metadata": {},
   "source": [
    "## *Portfolio analyses:*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12453f",
   "metadata": {},
   "source": [
    "### **Correlation analysis (NOT DONE YET)**\n",
    "#### Inputs\n",
    "- Pull time 5yr price time series, or the longest length in the portfolio\n",
    "    * Note: This will not work as well for IPOs... exclude outliers more than X Stds from the next shortest time series (mention in output)   \n",
    "    \n",
    "#### Outputs\n",
    "- Output matrix into report\n",
    "- Associate correlation coefficient ranges to risk label, and recomendations\n",
    "    * What % of equity is highly correlated, % somewhat correlated, % not correlated, % inversely correlated, % strongly inversely correlated\n",
    "- Save arrays and use for recomendation algorithm at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c3ffa5-134f-4c8b-bee2-51a08ee2a79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other portfolio analyses: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b97579-52ab-4e5b-a39b-8d20fedbbe88",
   "metadata": {},
   "source": [
    "# **Single stock analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab00d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy \n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import FundamentalAnalysis as fa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f7aee3-9f82-49af-9b65-192e85d43656",
   "metadata": {},
   "source": [
    "### Load in net income data for a given ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89325c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = \"AAPL\"\n",
    "api_key = \"7dead7d8a15b90bbadab8eeedfa59bd0\"\n",
    "last_year = 2020\n",
    "\n",
    "grab_data = False\n",
    "\n",
    "if grab_data == True:\n",
    "    cash_flow_statement_annually = fa.cash_flow_statement(ticker, api_key, period=\"annual\")\n",
    "    cash_flow = cash_flow_statement_annually.transpose()\n",
    "    cash_flow = cash_flow.rename_axis('year')\n",
    "    cash_flow.to_csv(r'C:\\Users\\xavie\\Portfolio_Rehab\\test_data\\%s_cash_flow.csv' % ticker)\n",
    "    \n",
    "else:\n",
    "    cash_flow = pd.read_csv(r'C:\\Users\\xavie\\Portfolio_Rehab\\test_data\\%s_cash_flow.csv' % ticker)\n",
    "\n",
    "revenue_np = cash_flow['netIncome'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd92dcc-3a1c-42e7-8b51-3412c7c63f0e",
   "metadata": {},
   "source": [
    "### Gain insight into 10 yr historical income growth (update to work with shorter life stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a12d8d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50674567fcca40b6b87bcabe161d8faf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual growth mean for 2012 to 2017: 15.425081467765938\n",
      "Annual growth mean for 2017 to 2020: 6.418114895858314\n",
      "5 year growth reduction percent: 58.3916953095492\n",
      "[3.7476460946063774, 2.1883140888427755]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "x = np.arange(last_year, last_year - 10, -1)\n",
    "y = revenue_np[:10]\n",
    "\n",
    "x_inc = x[::-1][1:] # Years increasing\n",
    "growth = []\n",
    "\n",
    "for i in range(len(y) - 1):\n",
    "    rate = ((y[i] - y[i + 1]) / y[i + 1]) * 100\n",
    "    growth.append(rate)\n",
    "\n",
    "growth_inc = growth.reverse() # Growth rate by year increasing   \n",
    "growth_array = np.array(growth)\n",
    "mean_growth = np.mean(growth_array)\n",
    "\n",
    "# Plot\n",
    "plt.plot(x_inc, growth_array)\n",
    "plt.hlines(mean_growth, xmin=x_inc[0], xmax=x_inc[-1], label='Mean growth rate percent = %s' % round(mean_growth, 2))\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('% annual revenue growth from previous year')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "first = growth_array[:5].mean()\n",
    "second = growth_array[5:].mean()\n",
    "print('Annual growth mean for %s to %s: %s' % (x_inc[0], x_inc[5], first))\n",
    "print('Annual growth mean for %s to %s: %s' % (x_inc[5], x_inc[-1], second))\n",
    "\n",
    "\n",
    "drops = ((first - second) / first)\n",
    "drops_percent = drops * 100\n",
    "print('5 year growth reduction percent: %s' % drops_percent )\n",
    "\n",
    "future_rates = [second * drops, second * drops**2] # [0-5 year growth rate, 5-10 year growth rate]\n",
    "print(future_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14b9150-3221-4e10-82c4-8570f556ebfb",
   "metadata": {},
   "source": [
    "## **Discounted cash flow valuation model** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922ea81f-1847-45be-8ea8-1c71bcb32e12",
   "metadata": {},
   "source": [
    "Upload historic p/e data csv and pull current quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bccb044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current AAPL price: 145.86\n"
     ]
    }
   ],
   "source": [
    "# Test w/ AAPL data!\n",
    "pe_data = r'C:\\Users\\xavie\\Portfolio_Rehab\\test_data\\aapl_pe_test.csv'\n",
    "\n",
    "# Pull up to date stock values\n",
    "quote_df = fa.quote(ticker, api_key)\n",
    "quote_df = quote_df.transpose()\n",
    "\n",
    "price = quote_df['price'].to_numpy()[0]\n",
    "market_cap = quote_df['marketCap'].to_numpy()[0]\n",
    "shares = market_cap / price\n",
    "\n",
    "print('Current %s price: %s' % (ticker, price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b95ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_cash_value(cash_flow, discount_rate, time_horizon, growth_rates, pe_data, pe_scenarios, optimism='low'):\n",
    "    \"\"\" This function automated Sven Carlin's discounted cash flow and outputs an intrinsic value based on optimism\n",
    "    and a modeled growth rate curve.\n",
    "    Inputs: Current cash flow, discount rate aka desired annualized return (float, percent), time_horizon in years (int),\n",
    "    a curve of length len(time_horizon) modeling future growth rates, an array storing historic P/E ratios that is used\n",
    "    to calculate the range of pe_values, optimism (str) either low, medium, or high (effects scenario probabilities)\"\"\"\n",
    "\n",
    "    opt_strs = []\n",
    "\n",
    "    # Define scenario outcome probabilities for terminal multiple\n",
    "    if optimism == 'low':\n",
    "        probs = [0.3, 0.6, 0.1]  # Worst case, medium case, best case\n",
    "\n",
    "    elif optimism == 'medium':\n",
    "        probs = [0.2, 0.6, 0.2]  # Worst case, medium case, best case\n",
    "\n",
    "    elif optimism == 'high':\n",
    "        probs = [0.1, 0.6, 0.3]  # Worst case, medium case, best case\n",
    "\n",
    "    else:\n",
    "        print('Optimism parameter error: must be low, medium, or high')\n",
    "\n",
    "    # Generate a forecasted growth rate curve\n",
    "    scenarios = ['pessimistic', 'medium', 'optimistic']\n",
    "    \n",
    "    # Calculate instrinsic value for each scenario\n",
    "    values = [] # Stores the value for each scenario\n",
    "    \n",
    "    for i, scene in enumerate(scenarios):\n",
    "        future_flow = [] # Stores the estimate future cash flows for each scenario\n",
    "        future_pv = [] # Stores estimate PV values for each scenario\n",
    "    \n",
    "        term_pe = pe_scenarios[i]\n",
    "        \n",
    "        for j, t in enumerate(range(0, time_horizon)):\n",
    "            if j <= 4:\n",
    "                rate = growth_rates[0]\n",
    "            else:\n",
    "                rate = growth_rates[1]\n",
    "                \n",
    "            if scene is 'optimistic':\n",
    "                rate = rate * 2\n",
    "            \n",
    "            if j == 0:\n",
    "                future_val = cash_flow * (1 + (rate / 100))\n",
    "                \n",
    "            else:\n",
    "                future_val = future_flow[j - 1] * (1 + (rate / 100))\n",
    "\n",
    "            future_flow.append(future_val)\n",
    "            \n",
    "            exp = -t - 1\n",
    "            pv = future_val * ((1 + discount_rate)**(exp))\n",
    "            future_pv.append(pv)\n",
    "        \n",
    "        terminal_value = future_flow[-1] * term_pe\n",
    "        term_pv = terminal_value * ((1 + discount_rate)**(exp))\n",
    "        future_pv.append(term_pv)\n",
    "        \n",
    "        values.append(sum(future_pv)) # FIGURE \n",
    "        \n",
    "    # Calculate weighted instrinsic value by optimism level\n",
    "    final_val = 0\n",
    "    for i, value in enumerate(values):\n",
    "        final_val += (value * probs[i])\n",
    "    \n",
    "    # Returns a list storing the optimism weighted average instrinsic value, and another list storing each scenario value\n",
    "    return [final_val, values] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbb1edeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historic P/E value array:\n",
      "[38.23 35.95 27.37 35.82 35.35 27.55 19.76 22.92 18.62 16.56 15.62 12.63\n",
      " 18.45 16.23 15.6  16.68 16.01 15.58 15.93 13.09 12.84 10.43 11.26 10.32\n",
      " 10.99 13.26 14.02 13.51 14.16 13.5  11.47 12.37 10.64  8.7   9.22 10.47\n",
      " 13.04 11.8  12.57  9.92 11.85 11.43 14.28 15.49 16.1  17.54 19.87 20.92\n",
      " 21.22 17.62 13.31 11.97 18.24 28.17 25.54 37.76 34.   30.08 25.61 26.53]\n",
      "\n",
      "Pessimistic, medium, and optimistic terminal multiple values\n",
      "[10.195359855957339, 18.099499999999995, 33.907780288085306]\n"
     ]
    }
   ],
   "source": [
    "if pe_data[-4:] == '.csv':\n",
    "        pe_df = pd.read_csv(pe_data)\n",
    "        pe_array = pe_df['pe_ratio'].to_numpy()\n",
    "\n",
    "else:\n",
    "    pe_array = pe_data\n",
    "\n",
    "print('Historic P/E value array:')\n",
    "print(pe_array)\n",
    "\n",
    "mean_val = np.nanmean(pe_array)\n",
    "std_val = np.nanstd(pe_array)\n",
    "\n",
    "optimist = mean_val + (2 * std_val)\n",
    "pessimist = mean_val - std_val\n",
    "\n",
    "scenario_pe = [pessimist, mean_val, optimist]\n",
    "\n",
    "print()\n",
    "print('Pessimistic, medium, and optimistic terminal multiple values')\n",
    "print(scenario_pe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fa0e5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired annualized return is 5.0 percent\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa22923e75444c7f9d049df89590628a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "test_rates = [i for i in range(1, 15)]\n",
    "scenarios = ['low', 'medium', 'high']\n",
    "desired_rate = 0.05\n",
    "print('Desired annualized return is %s percent' % round(float(desired_rate * 100), 2))\n",
    "labels = ['Pessimistic', 'Neutral', 'Optimistic']\n",
    "prices = []\n",
    "\n",
    "for i, opt in enumerate(scenarios):\n",
    "    comps = []\n",
    "    price_sub = []\n",
    "    for initial in test_rates:\n",
    "        rates = [initial, initial / 2]\n",
    "        out = discounted_cash_value(revenue_np[0], desired_rate, 10, rates, pe_data, scenario_pe, optimism=opt)\n",
    "        i_price = float(out[0] / shares) # Intrinsic value share price\n",
    "        price_sub.append(i_price)\n",
    "        comp = ((out[0] - market_cap) / market_cap) * 100\n",
    "        comps.append(comp)\n",
    "\n",
    "    comps_np = np.asarray(comps)\n",
    "    initial_np = np.asarray(test_rates)\n",
    "    plt.plot(initial_np, comps_np, label=labels[i])\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.hlines(0, xmin=np.min(initial_np), xmax=np.max(initial_np))\n",
    "    plt.xlabel('Initial growth rate %, halves after 5 years')\n",
    "    plt.ylabel('Intrinsic value percent +/- current value for return')\n",
    "    plt.xlim(np.min(initial_np), np.max(initial_np))\n",
    "    \n",
    "    prices.append(price_sub)\n",
    "\n",
    "\n",
    "# Make plot again and figure out how to make a clean git commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76c6b604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df8ca54578f435d83402fbd5982f8f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "for i, sub in enumerate(prices):\n",
    "    price_np = np.asarray(sub)\n",
    "\n",
    "    plt.plot(initial_np, price_np, label=labels[i])\n",
    "    if i == 1:\n",
    "        plt.hlines(price, xmin=np.min(initial_np), xmax=np.max(initial_np), label='Current price')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.xlabel('Initial growth rate %, halves after 5 years')\n",
    "    plt.ylabel('Fair price for %s percent annual return' % round(float(desired_rate * 100), 2))\n",
    "    plt.xlim(np.min(initial_np), np.max(initial_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c95a40-aff6-46b1-af88-0486b0d221df",
   "metadata": {},
   "source": [
    "## **Intrest rates analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "179edee8-a311-4ad1-81a2-65b369073de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pe_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2006-12-31</th>\n",
       "      <td>26.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-03-31</th>\n",
       "      <td>25.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-06-30</th>\n",
       "      <td>30.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-09-30</th>\n",
       "      <td>34.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-12-31</th>\n",
       "      <td>37.76</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pe_ratio\n",
       "Date                \n",
       "2006-12-31     26.53\n",
       "2007-03-31     25.61\n",
       "2007-06-30     30.08\n",
       "2007-09-30     34.00\n",
       "2007-12-31     37.76"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in ticker historic P/E ratio data, and flip from macro axis format\n",
    "pe_df_inc = pe_df.reindex(index=pe_df.index[::-1])\n",
    "\n",
    "start = pd.to_datetime(pe_df_inc['Date'].to_numpy()[0])\n",
    "end = pd.to_datetime(pe_df_inc['Date'].to_numpy()[-1])\n",
    "\n",
    "pe_df_inc['Date'] = pd.to_datetime(pe_df_inc['Date'])\n",
    "pe_df_inc.set_index('Date', inplace=True)\n",
    "\n",
    "pe_df_inc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a8c6769-84dc-4104-a666-47d29c612207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in historic US Treasury intrest rates\n",
    "import datetime as dt\n",
    "intrest_df = pd.read_csv(r'C:\\Users\\xavie\\Portfolio_Rehab\\test_data\\intrest_rates.csv')\n",
    "intrest_df.head()\n",
    "intrest_df['Date'] = pd.to_datetime(intrest_df['Date'])\n",
    "intrest_df.set_index('Date', inplace=True)\n",
    "\n",
    "intrest_df = intrest_df[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c85eec9-6d33-41f9-a47d-ae6c69861a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pe_ratio</th>\n",
       "      <th>rate</th>\n",
       "      <th>std_pe_ratio</th>\n",
       "      <th>std_rate</th>\n",
       "      <th>covariance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-01-01</th>\n",
       "      <td>26.53</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1.274634</td>\n",
       "      <td>3.337141</td>\n",
       "      <td>4.253634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-02</th>\n",
       "      <td>26.53</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1.274634</td>\n",
       "      <td>3.337141</td>\n",
       "      <td>4.253634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-03</th>\n",
       "      <td>26.53</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1.274634</td>\n",
       "      <td>3.337141</td>\n",
       "      <td>4.253634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-04</th>\n",
       "      <td>26.53</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1.274634</td>\n",
       "      <td>3.337141</td>\n",
       "      <td>4.253634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-01-05</th>\n",
       "      <td>26.53</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1.274634</td>\n",
       "      <td>3.337141</td>\n",
       "      <td>4.253634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            pe_ratio  rate  std_pe_ratio  std_rate  covariance\n",
       "Date                                                          \n",
       "2007-01-01     26.53  6.25      1.274634  3.337141    4.253634\n",
       "2007-01-02     26.53  6.25      1.274634  3.337141    4.253634\n",
       "2007-01-03     26.53  6.25      1.274634  3.337141    4.253634\n",
       "2007-01-04     26.53  6.25      1.274634  3.337141    4.253634\n",
       "2007-01-05     26.53  6.25      1.274634  3.337141    4.253634"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make both dataframes at the daily scale, then merge. Create standardized values\n",
    "pe_by_day = pe_df_inc.asfreq(pd.offsets.BDay(), method=\"pad\")\n",
    "int_by_day = intrest_df.asfreq(pd.offsets.BDay(), method=\"pad\")\n",
    "\n",
    "aligned = pe_by_day.merge(int_by_day, left_on='Date', right_on='Date')\n",
    "\n",
    "std_cols = []\n",
    "for col in aligned.columns:\n",
    "    name = 'std_%s' % col\n",
    "    mean = aligned[col].mean()\n",
    "    std = aligned[col].std()\n",
    "    aligned[name] = (aligned[col] - mean) / std\n",
    "    std_cols.append(name)\n",
    "    \n",
    "aligned['covariance'] = aligned[std_cols[0]] * aligned[std_cols[1]]\n",
    "aligned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77df820e-82ab-4b25-93ea-0370ca991be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized P/E ratio and interest rates:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b5eabfc4514b038eac4c34d058f522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons correlation coefficient: 0.42742683927857666, P value: 5.840488747699123e-167\n",
      "Monthly change in P/E ratio and interest rates\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c97eecb91c745cebc733cdca28bba26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearsons correlation coefficient: 0.18272207600888235, P value: 6.204574376835259e-29\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "# Plot standardized P/E ratio and interest rate values\n",
    "print('Standardized P/E ratio and interest rates:')\n",
    "aligned[std_cols].plot()\n",
    "plt.ylabel('Standardized value')\n",
    "\n",
    "\n",
    "# Calculate correlation\n",
    "pears = stats.pearsonr(aligned[std_cols[0]], aligned[std_cols[1]])\n",
    "print('Pearsons correlation coefficient: %s, P value: %s' % pears)\n",
    "\n",
    "# Investigate whether the change in standardized interest rates or P/E ratios are related\n",
    "print('Monthly change in P/E ratio and interest rates')\n",
    "deriv = aligned[std_cols].diff(90)\n",
    "deriv = deriv.dropna(how='any')\n",
    "deriv.to_csv(r'C:\\Users\\xavie\\Portfolio_Rehab\\test_data\\deriv_test.csv')\n",
    "deriv['covariance'] = deriv[std_cols[0]] * deriv[std_cols[1]]\n",
    "deriv[std_cols].plot()\n",
    "plt.ylabel('Change in standardized value (90 day steps)')\n",
    "\n",
    "# Calculate correlation\n",
    "d_pears = stats.pearsonr(deriv[std_cols[0]], deriv[std_cols[1]])\n",
    "print('Pearsons correlation coefficient: %s, P value: %s' % d_pears)\n",
    "\n",
    "interest_rate_outputs = [pears, d_pears] # outputs for the position analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9cb104-9c69-4e98-a176-a2287e0c9dab",
   "metadata": {},
   "source": [
    "### **Intrest rate related position risk**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7171a6-4906-418b-95c3-7c05fa3f33a7",
   "metadata": {},
   "source": [
    "Interpretes the risks associated with current position in relation to the DCF and interest rate analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3808b317-f271-4b9a-b482-c927226c5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_basis = 140\n",
    "cost_basis_pe = 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a745181-b066-44ae-8e80-65747b16b11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interest_rate_risk(aligned_df, pe_array, analysis_list):\n",
    "    \"\"\"Inputs: Current US treasury interest rate, P/E ratio w/ cost basis, list output of the interest rate analysis\n",
    "    Outputs: Strings describing the estimated risk associated with rising interest rates\"\"\"\n",
    "    \n",
    "    print('Disclaimer! Increasing interest rates are associated with falling P/E multiples!')\n",
    "    print('This analysis identifies if the input position is at elevated risk due to historical interest rate associated \\\n",
    "P/E multiple contractions')\n",
    "    print()\n",
    "    \n",
    "    thresholds = [0, 0.15, 0.3, 0.7]\n",
    "    interps = ['Slight', 'Some', 'Moderate', 'Strong']\n",
    "    tests = ['P/E ratio correlation w/ interest rates', 'change in P/E ratio correlation w/ interest rates']\n",
    "    elevated = [False, False]\n",
    "    \n",
    "    # Estimate risk posed by negative interest rate covariance, correlation, derivative covariance, and derivative correlation\n",
    "    corr_list = ['' for i in analysis_list] # Stores risk strings based on on\n",
    "    comb_risk = ['' for i in analysis_list] \n",
    "    \n",
    "    for j, metric in enumerate(analysis_list):\n",
    "        for i, thresh in enumerate(thresholds):  \n",
    "            val = metric[0]\n",
    "            \n",
    "            if val > 0 or metric[1] > 0.05:\n",
    "                corr_list[j] = 'Low'\n",
    "                \n",
    "            elif val <= -thresh:\n",
    "                corr = interps[i]\n",
    "                corr_list[j] = corr\n",
    "            \n",
    "    # Print out descriptions of the correlations \n",
    "    print()\n",
    "    print('Correlations:')\n",
    "    if corr_list[0] == 'Low':\n",
    "        print('%s historic P/E ratios are NOT negatively correlated w/ interest rates, or the reltionship is not statistically \\\n",
    "significant at the p < 0.05 level.' % ticker)\n",
    "    else:\n",
    "        index = interps.index(corr_list[0])\n",
    "        print('%s historic P/E has a %s (-%s > pearsons coeff. > -%s), statistically significant (p < 0.05), NEGATIVE correlation \\\n",
    "w/ interest rates.' % (ticker, corr_list[0], thresholds[index], thresholds[index + 1]))\n",
    "        \n",
    "    if corr_list[1] == 'Low':\n",
    "        print('Time periods w/ increasing interest rates are NOT correlated w/ decreasing %s P/E ratio, or the reltionship is not statistically \\\n",
    "significant at the p < 0.05 level.' % ticker)\n",
    "    \n",
    "    else:\n",
    "        index = interps.index(corr_list[1])\n",
    "        print('Time periods w/ INCREASING interest rates have a %s (-%s > pearsons coeff. > -%s), statistically significant (p < 0.05), \\\n",
    "correlation w/ DECREASING %s P/E ratio.' % (corr_list[1], thresholds[index], thresholds[index + 1], ticker))\n",
    "    \n",
    "    # Compare current interest rates and P/E to historic distribution (0.5 STD threshold)\n",
    "    print('Comaprison to historic values:')\n",
    "    cols = ['rate', 'std_rate', 'pe_ratio', 'std_pe_ratio']\n",
    "    \n",
    "    val_list = []\n",
    "    for i in range(len(cols)):\n",
    "        val_list.append(aligned_df[cols[i]].to_numpy()[-1])\n",
    "    \n",
    "    if val_list[1] < -0.5:\n",
    "        elevated[0] = True\n",
    "        print('The current interests rates of %s is more than 0.5 standard deviations less than the historic mean: \\\n",
    "Z = %s' % (val_list[0], val_list[1]))\n",
    "            \n",
    "    if val_list[3] > 0.5:\n",
    "        elevated[1] = True\n",
    "        print('%ss current P/E ratio of %s is more than 0.5 standard deviations greater than the historic mean: \\\n",
    "Z = %s' % (ticker, val_list[2], val_list[3]))\n",
    "    \n",
    "    print()\n",
    "    print('Intepreting findings: If P/E multiple and interest rates are negatively correlated AND intrest rates are low while current \\\n",
    "valuation (P/E) is high, then there is significant risk associated with a mean reversion following changes to US treasury interest rates')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7212303c-814d-4c3f-933a-c0ed15d6f36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disclaimer! Increasing interest rates are associated with falling P/E multiples!\n",
      "This analysis identifies if the input position is at elevated risk due to historical interest rate associated P/E multiple contractions\n",
      "\n",
      "\n",
      "Correlations:\n",
      "AAPL historic P/E has a Moderate (-0.3 > pearsons coeff. > -0.7), statistically significant (p < 0.05), NEGATIVE correlation w/ interest rates.\n",
      "Time periods w/ increasing interest rates are NOT correlated w/ decreasing AAPL P/E ratio, or the reltionship is not statistically significant at the p < 0.05 level.\n",
      "Comaprison to historic values:\n",
      "The current interests rates of 0.25 is more than 0.5 standard deviations less than the historic mean: Z = -0.8703661364792227\n",
      "AAPLs current P/E ratio of 27.37 is more than 0.5 standard deviations greater than the historic mean: Z = 1.3917767058042434\n",
      "\n",
      "Intepreting findings: If P/E multiple and interest rates are negatively correlated AND intrest rates are low while current valuation (P/E) is high, then there is significant risk associated with a mean reversion following changes to US treasury interest rates\n"
     ]
    }
   ],
   "source": [
    "interest_rate_risk(aligned, pe_array, [(-0.55, 0.000001), (-0.2, 0.4)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8860c6e1-f1a1-45f2-a085-909ff4fd35b0",
   "metadata": {},
   "source": [
    "## **Economic Value Added (EVA) analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d3a1d3-1ac9-4cb4-91f4-b032754046fd",
   "metadata": {},
   "source": [
    "#### *What is EVA?*\n",
    "- Economic value added (EVA), also known as economic profit, aims to calculate the true economic profit of a company.\n",
    "- EVA is used to measure the value a company generates from funds invested in it.\n",
    "\n",
    "#### *Advantages and Disadvantages of EVA*\n",
    "- EVA assesses the performance of a company and its management through the idea that a business is only profitable when it creates wealth and returns for shareholders, thus requiring performance above a company's cost of capital.\n",
    "- EVA as a performance indicator is very useful. The calculation shows how and where a company created wealth, through the inclusion of balance sheet items. This forces managers to be aware of assets and expenses when making managerial decisions.\n",
    "- However, the EVA calculation relies heavily on the amount of invested capital and is best used for asset-rich companies that are stable or mature. Companies with intangible assets, such as technology businesses, may not be good candidates for an EVA evaluation. \n",
    "\n",
    "Source: <https://www.investopedia.com/terms/e/eva.asp>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23e8d11e-c843-4ff6-bafd-565799ad1843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in earning data \n",
    "grab_data = True\n",
    "\n",
    "if grab_data == True:\n",
    "    # Pull earnings report\n",
    "    earnings_pull = fa.income_statement(ticker, api_key, period=\"annual\")\n",
    "    earnings_df = earnings_pull.transpose()\n",
    "    earnings_df = earnings_df.rename_axis('year')\n",
    "    earnings_df.to_csv(r'C:\\Users\\xavie\\Portfolio_Rehab\\test_data\\%s_earnings.csv' % ticker)\n",
    "    \n",
    "    # Pull balance sheet\n",
    "    balance_pull = fa.balance_sheet_statement(ticker, api_key, period=\"annual\")\n",
    "    balance_df = balance_pull.transpose()\n",
    "    balance_df = balance_df.rename_axis('year')\n",
    "    balance_df.to_csv(r'C:\\Users\\xavie\\Portfolio_Rehab\\test_data\\%s_balance.csv' % ticker)\n",
    "    \n",
    "    fa.financial_ratios \n",
    "    \n",
    "else:\n",
    "    earnings_df = pd.read_csv(r'C:\\Users\\xavie\\Portfolio_Rehab\\test_data\\%s_earnings.csv' % ticker)\n",
    "    balance_df = pd.read_csv(r'C:\\Users\\xavie\\Portfolio_Rehab\\test_data\\%s_balance.csv' % ticker)\n",
    "\n",
    "\n",
    "total_assets = balance_df['totalAssets'].to_numpy()\n",
    "operating_income = earnings_df['operatingIncome'].to_numpy()\n",
    "tax_losses = earnings_df['incomeTaxExpense'].to_numpy()\n",
    "intrest_expense = earnings_df['interestExpense'].to_numpy()\n",
    "years = earnings_df.index.to_numpy()\n",
    "\n",
    "# Make data frame for EVA analysis\n",
    "eva_df = pd.DataFrame.from_dict({'year': years, 'total_assets' : total_assets, \n",
    "                                    'operating_income' : operating_income, 'tax_losses' : tax_losses, 'intrest_expense' : intrest_expense}) \n",
    "eva_df['tax_rate'] =  eva_df['tax_losses'] / eva_df['operating_income']\n",
    "eva_df['NOPAT'] = eva_df['operating_income'] - eva_df['tax_losses'] - (eva_df['intrest_expense'] * eva_df['tax_rate'])\n",
    "eva_df = eva_df.reindex(index=eva_df.index[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36b332ce-5d69-4717-9e15-d56958133d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3dc0a32ba74bf19bf3a80d1f292c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Year')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "# Define desired annualized return\n",
    "desired_rates = [0.05, 0.07, 0.10, 0.15, 0.2] # A list of annualized returns to calculate EVA for\n",
    "\n",
    "# Define farthest back year to analyze\n",
    "back_year = 2006\n",
    "\n",
    "eva_df = eva_df.loc[eva_df['year'].astype(float) >= back_year]\n",
    "x = eva_df['year'].to_numpy()\n",
    "for rate in desired_rates:\n",
    "    temp_eva = eva_df.copy()\n",
    "    temp_eva['EVA'] = temp_eva['NOPAT'] - (temp_eva['total_assets'] * rate)\n",
    "    evas = temp_eva['EVA'].to_numpy()\n",
    "    plt.plot(x, evas, label='Annual return percent: %s' % round(float(rate * 100), 2))\n",
    "    \n",
    "plt.title('Economic Value Added (EVA) w/ expected return scenarios')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "cb67a987-521d-46bf-918e-313c2e808f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sales and plot EVA/Sales, print out recomendation based on whether they are persistant above 5%\n",
    "\n",
    "# A 5% EVA Margin can be used as an indicator for a \"good\" company, whereas the persistence of a 5%+ EVA Margin for 10 years \n",
    "#...makes a company great and thus \"moaty\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ac2ba-6694-4e92-8c50-be3311dda20d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
